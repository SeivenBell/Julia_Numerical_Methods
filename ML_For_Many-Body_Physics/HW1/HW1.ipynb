{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 30, 30]) torch.Size([7000])\n",
      "torch.Size([2000, 30, 30]) torch.Size([2000])\n",
      "torch.Size([1000, 30, 30]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "x = np.loadtxt(\"x_L30.txt\").reshape(\n",
    "    -1, 30, 30\n",
    ")  # Assuming the data is already in this shape\n",
    "y = np.loadtxt(\"y_L30.txt\")\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x_tensor, y_tensor, test_size=0.3, random_state=42\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=(1 / 3), random_state=42\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the shapes\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "x_train = x_train.float()\n",
    "x_val = x_val.float()\n",
    "x_test = x_test.float()\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingModelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IsingModelNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(30 * 30, 200), nn.ReLU(), nn.Linear(200, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = IsingModelNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Acc: 76.4%, Val Acc: 73.2%\n",
      "Epoch 2, Train Acc: 77.2%, Val Acc: 74.3%\n",
      "Epoch 3, Train Acc: 82.8%, Val Acc: 80.2%\n",
      "Epoch 4, Train Acc: 87.7%, Val Acc: 86.5%\n",
      "Epoch 5, Train Acc: 90.6%, Val Acc: 89.1%\n",
      "Epoch 6, Train Acc: 92.1%, Val Acc: 90.9%\n",
      "Epoch 7, Train Acc: 93.6%, Val Acc: 92.4%\n",
      "Epoch 8, Train Acc: 93.5%, Val Acc: 92.3%\n",
      "Epoch 9, Train Acc: 94.0%, Val Acc: 92.8%\n",
      "Epoch 10, Train Acc: 96.5%, Val Acc: 95.1%\n",
      "Epoch 11, Train Acc: 96.8%, Val Acc: 95.2%\n",
      "Epoch 12, Train Acc: 97.5%, Val Acc: 96.2%\n",
      "Epoch 13, Train Acc: 97.8%, Val Acc: 96.3%\n",
      "Epoch 14, Train Acc: 98.2%, Val Acc: 96.5%\n",
      "Epoch 15, Train Acc: 98.3%, Val Acc: 96.6%\n",
      "Epoch 16, Train Acc: 98.3%, Val Acc: 96.4%\n",
      "Epoch 17, Train Acc: 98.7%, Val Acc: 97.0%\n",
      "Epoch 18, Train Acc: 98.7%, Val Acc: 97.1%\n",
      "Epoch 19, Train Acc: 98.9%, Val Acc: 97.0%\n",
      "Epoch 20, Train Acc: 99.0%, Val Acc: 97.2%\n",
      "Epoch 21, Train Acc: 98.9%, Val Acc: 97.2%\n",
      "Epoch 22, Train Acc: 97.5%, Val Acc: 95.4%\n",
      "Epoch 23, Train Acc: 99.1%, Val Acc: 97.2%\n",
      "Epoch 24, Train Acc: 98.6%, Val Acc: 96.4%\n",
      "Epoch 25, Train Acc: 99.3%, Val Acc: 97.3%\n",
      "Epoch 26, Train Acc: 99.3%, Val Acc: 97.3%\n",
      "Epoch 27, Train Acc: 99.4%, Val Acc: 97.0%\n",
      "Epoch 28, Train Acc: 99.5%, Val Acc: 97.2%\n",
      "Epoch 29, Train Acc: 99.4%, Val Acc: 97.3%\n",
      "Epoch 30, Train Acc: 99.2%, Val Acc: 97.0%\n",
      "Epoch 31, Train Acc: 99.5%, Val Acc: 97.3%\n",
      "Epoch 32, Train Acc: 99.5%, Val Acc: 97.3%\n",
      "Epoch 33, Train Acc: 99.5%, Val Acc: 97.2%\n",
      "Epoch 34, Train Acc: 99.5%, Val Acc: 97.5%\n",
      "Epoch 35, Train Acc: 99.6%, Val Acc: 97.3%\n",
      "Epoch 36, Train Acc: 99.5%, Val Acc: 97.3%\n",
      "Epoch 37, Train Acc: 99.4%, Val Acc: 97.2%\n",
      "Epoch 38, Train Acc: 99.6%, Val Acc: 97.4%\n",
      "Epoch 39, Train Acc: 99.6%, Val Acc: 97.2%\n",
      "Epoch 40, Train Acc: 99.4%, Val Acc: 97.1%\n",
      "Epoch 41, Train Acc: 99.6%, Val Acc: 97.2%\n",
      "Epoch 42, Train Acc: 99.6%, Val Acc: 97.5%\n",
      "Epoch 43, Train Acc: 99.7%, Val Acc: 97.5%\n",
      "Epoch 44, Train Acc: 99.7%, Val Acc: 97.3%\n",
      "Epoch 45, Train Acc: 99.7%, Val Acc: 97.5%\n",
      "Epoch 46, Train Acc: 99.7%, Val Acc: 97.3%\n",
      "Epoch 47, Train Acc: 99.7%, Val Acc: 97.5%\n",
      "Epoch 48, Train Acc: 99.8%, Val Acc: 97.3%\n",
      "Epoch 49, Train Acc: 99.7%, Val Acc: 97.4%\n",
      "Epoch 50, Train Acc: 99.7%, Val Acc: 96.9%\n",
      "Final Validation Accuracy: 97.5%\n",
      "Test Accuracy: 97.1%, Test Loss: 0.1455\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            total_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    avg_accuracy = 100 * total_correct / num_samples\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_accuracy, avg_loss\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "best_val_acc = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, criterion, optimizer)\n",
    "    train_acc, train_loss = evaluate(train_loader, model, criterion)\n",
    "    val_acc, val_loss = evaluate(val_loader, model, criterion)\n",
    "    print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.1f}%, Val Acc: {val_acc:.1f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = model.state_dict()  # Save the best model\n",
    "\n",
    "# After training, load the best model and evaluate on test data\n",
    "model.load_state_dict(best_model)\n",
    "test_acc, test_loss = evaluate(test_loader, model, criterion)\n",
    "print(f\"Final Validation Accuracy: {best_val_acc:.1f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.1f}%, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m T_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_L30.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Assuming T_test is a tensor or array that contains the temperatures for the test set\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m T_test_np \u001b[38;5;241m=\u001b[39m T_test\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mT_test\u001b[49m, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m T_test\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare data structure to store temperature-wise results\u001b[39;00m\n\u001b[0;32m      6\u001b[0m temp_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     temp\u001b[38;5;241m.\u001b[39mitem(): {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)}\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39munique(T_test)\n\u001b[0;32m      9\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T_test' is not defined"
     ]
    }
   ],
   "source": [
    "T_data = np.loadtxt(\"T_L30.txt\")\n",
    "# Assuming T_test is a tensor or array that contains the temperatures for the test set\n",
    "\n",
    "T_test_np = T_test.numpy() if isinstance(T_test, torch.Tensor) else T_test\n",
    "\n",
    "# Prepare data structure to store temperature-wise results\n",
    "temp_dict = {\n",
    "    temp.item(): {\"correct\": 0, \"total\": 0, \"output_sum\": np.zeros(2)}\n",
    "    for temp in torch.unique(T_test)\n",
    "}\n",
    "\n",
    "# Evaluate the model for each sample in the test set and collate results by temperature\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y, temp in zip(\n",
    "        test_loader.dataset.tensors[0], test_loader.dataset.tensors[1], T_test_np\n",
    "    ):\n",
    "        output = model(X.unsqueeze(0))  # Add batch dimension\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct = (pred == y).item()\n",
    "\n",
    "        temp_dict[temp][\"correct\"] += correct\n",
    "        temp_dict[temp][\"total\"] += 1\n",
    "        temp_dict[temp][\"output_sum\"] += output.softmax(dim=1).squeeze(0).numpy()\n",
    "\n",
    "# Calculate average accuracy and average output for each temperature\n",
    "avg_accuracy_by_temp = {\n",
    "    temp: data[\"correct\"] / data[\"total\"] for temp, data in temp_dict.items()\n",
    "}\n",
    "avg_output_by_temp = {\n",
    "    temp: data[\"output_sum\"] / data[\"total\"] for temp, data in temp_dict.items()\n",
    "}\n",
    "\n",
    "# Sort temperatures for plotting\n",
    "sorted_temps = sorted(avg_accuracy_by_temp.keys())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Average accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    sorted_temps, [avg_accuracy_by_temp[temp] for temp in sorted_temps], marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"Temperature (T/J)\")\n",
    "plt.ylabel(\"Average Accuracy\")\n",
    "plt.title(\"Average Accuracy by Temperature\")\n",
    "\n",
    "# Average output neuron plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for neuron_index in range(2):\n",
    "    plt.plot(\n",
    "        sorted_temps,\n",
    "        [avg_output_by_temp[temp][neuron_index] for temp in sorted_temps],\n",
    "        marker=\"o\",\n",
    "        label=f\"Neuron {neuron_index}\",\n",
    "    )\n",
    "plt.xlabel(\"Temperature (T/J)\")\n",
    "plt.ylabel(\"Average Neuron Output\")\n",
    "plt.title(\"Average Output Neuron Value by Temperature\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(dataloader, model, loss_fn):\n",
    "#     model.eval()\n",
    "#     total_loss, total_correct = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             pred = model(X)\n",
    "#             total_loss += loss_fn(pred, y).item()\n",
    "#             total_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "#     num_samples = len(dataloader.dataset)\n",
    "#     avg_accuracy = 100 * total_correct / num_samples\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     return avg_accuracy, avg_loss\n",
    "\n",
    "\n",
    "# # After training and validation loop\n",
    "# test_acc, test_loss = evaluate(test_loader, model, criterion)\n",
    "# print(f\"Test Accuracy: {test_acc:.1f}%, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# print(f\"Final Validation Accuracy: {val_acc:.1f}%\")\n",
    "# print(f\"Test Accuracy: {test_acc:.1f}%\")\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# epochs = 50\n",
    "# best_val_acc = 0\n",
    "# best_model = None\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train(train_loader, model, criterion, optimizer)\n",
    "#     train_acc, train_loss = evaluate(train_loader, model, criterion)\n",
    "#     val_acc, val_loss = evaluate(val_loader, model, criterion)\n",
    "\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         best_model = model.state_dict()  # Save the best model\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.1f}%, Val Acc: {val_acc:.1f}%\")\n",
    "\n",
    "# # After training, load the best model and evaluate on test data\n",
    "# model.load_state_dict(best_model)\n",
    "# test_acc, test_loss = evaluate(test_loader, model, criterion)\n",
    "# print(f\"Final Validation Accuracy: {best_val_acc:.1f}%\")\n",
    "# print(f\"Test Accuracy: {test_acc:.1f}%, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             pred = model(X)\n",
    "#             total_loss += loss_fn(pred, y).item()\n",
    "#             total_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "#     num_samples = len(dataloader.dataset)\n",
    "#     avg_accuracy = 100 * total_correct / num_samples\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     return avg_accuracy, avg_loss\n",
    "\n",
    "\n",
    "# # After training and validation loop\n",
    "# test_acc, test_loss = evaluate(test_loader, model, criterion)\n",
    "# print(f\"Test Accuracy: {test_acc:.1f}%, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# print(f\"Final Validation Accuracy: {val_acc:.1f}%\")\n",
    "# print(f\"Test Accuracy: {test_acc:.1f}%\")\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# epochs = 50\n",
    "# best_val_acc = 0\n",
    "# best_model = None\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train(train_loader, model, criterion, optimizer)\n",
    "#     train_acc, train_loss = evaluate(train_loader, model, criterion)\n",
    "#     val_acc, val_loss = evaluate(val_loader, model, criterion)\n",
    "\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         best_model = model.state_dict()  # Save the best model\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.1f}%, Val Acc: {val_acc:.1f}%\")\n",
    "\n",
    "# # After training, load the best model and evaluate on test data\n",
    "# model.load_state_dict(best_model)\n",
    "# test_acc, test_loss = evaluate(test_loader, model, criterion)\n",
    "# print(f\"Final Validation Accuracy: {best_val_acc:.1f}%\")\n",
    "# print(f\"Test Accuracy: {test_acc:.1f}%, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code:\n",
    "\n",
    "# def plot_metrics(metrics, title=\"Model Performance\"):\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for label, data in metrics[\"accuracy\"].items():\n",
    "#         plt.plot(data, label=label)\n",
    "#     plt.title(\"Accuracy vs. Epochs\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     for label, data in metrics[\"loss\"].items():\n",
    "#         plt.plot(data, label=label)\n",
    "#     plt.title(\"Loss vs. Epochs\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.suptitle(title)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# weight_decays = [0, 0.001, 0.01, 0.1]\n",
    "# metrics = {\"accuracy\": {}, \"loss\": {}}\n",
    "\n",
    "# for decay in weight_decays:\n",
    "#     model = IsingModelNN()  # Reinitialize model\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=decay)\n",
    "#     train_accuracies, val_accuracies = [], []\n",
    "#     train_losses, val_losses = [], []\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         train(train_loader, model, criterion, optimizer)\n",
    "#         train_acc, train_loss = validate(train_loader, model, criterion)\n",
    "#         val_acc, val_loss = validate(val_loader, model, criterion)\n",
    "#         train_accuracies.append(train_acc)\n",
    "#         val_accuracies.append(val_acc)\n",
    "#         train_losses.append(train_loss)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "#     metrics[\"accuracy\"][f\"wd={decay}\"] = val_accuracies\n",
    "#     metrics[\"loss\"][f\"wd={decay}\"] = val_losses\n",
    "\n",
    "# plot_metrics(metrics, \"Impact of L2 Regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weight decay factor added; start with a small value\n",
    "# weight_decay = 0.01\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "# # \"SGD\": optim.SGD(model.parameters(), lr=0.01),\n",
    "# # \"SGD with Momentum\": optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "# # \"Adam\": optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# def train(dataloader, model, loss_fn, optimizer):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     model.train()\n",
    "#     for batch, (X, y) in enumerate(dataloader):\n",
    "#         pred = model(X)\n",
    "#         loss = loss_fn(pred, y)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "# def validate(dataloader, model, loss_fn):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     model.eval()\n",
    "#     loss, correct = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             pred = model(X)\n",
    "#             loss += loss_fn(pred, y).item()\n",
    "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "#     loss /= num_batches\n",
    "#     correct /= size\n",
    "#     return 100 * correct, loss\n",
    "\n",
    "\n",
    "# epochs = 50\n",
    "# for epoch in range(epochs):\n",
    "#     train(train_loader, model, criterion, optimizer)\n",
    "#     train_acc, train_loss = validate(train_loader, model, criterion)\n",
    "#     val_acc, val_loss = validate(val_loader, model, criterion)\n",
    "#     print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.1f}%, Val Acc: {val_acc:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
